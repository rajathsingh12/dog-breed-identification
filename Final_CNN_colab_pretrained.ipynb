{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t9S0GBeKUncb"
   },
   "source": [
    "# **Dog-Breed Identification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "estD08MHUnch"
   },
   "source": [
    " *The dataset is downloaded from the following: https://www.kaggle.com/c/dog-breed-identification/data*\n",
    " \n",
    " **Why did I choose this particular challenge?**\n",
    " \n",
    " The reason that I chose this dataset is because it has 120 different classes of breeds among dogs. That is, it is 120 class classification problem and I have never delt with a classification model of these many classes before. So I took this opportunity to take on a challenge and see how I could push myself to go about it. I realized that the datasets include a huge number of images of different breed of dogs (120) and that it would require a decent amount of computation power to train on all the images which I couldn't possibly do in my laptop, so I used Google Colab to train the model. \n",
    "\n",
    "\n",
    " **Challenges Faced and Approach**\n",
    " \n",
    "-  The first challenge, was to figure out how to train the model on such a huge dataset, for which I used Google Colab.\n",
    " \n",
    "-  The second challenge that I came across after I trained my own model, was validation accuracy. to resolve this problem, I used the transfer learning approach where I used a pre-trained model on top of my own model to achieve better accuracy.\n",
    "\n",
    "- Consequently, I opted to use Jupyter Notebooks so that I could document everything, each step of the project, along with the code to clearly illustrate how I went about the project with the hope that it makes it easy to follow my approach to this problem and evaluate my performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DC-hvL3DHpYs"
   },
   "source": [
    "## Step 1 - Importing all the important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YSoa-Wx7HpYx"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PNcOR5BuUnc5"
   },
   "source": [
    "- #### The following code is to link Gdrive to Google Colab and to unzip the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1jE_g9K8OofT"
   },
   "outputs": [],
   "source": [
    "# To Authorize your Gdrive to Google Colab\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1z3mq1YJRuDC"
   },
   "outputs": [],
   "source": [
    "# To unzip from the drive\n",
    "#!unzip -q \"drive/My Drive/dataset/test.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eRu1RabYTnCY"
   },
   "outputs": [],
   "source": [
    "# To unzip from the drive\n",
    "#!unzip -q \"drive/My Drive/dataset/train.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oqzxM7pXHpZF"
   },
   "source": [
    "## Step 2 - Data Preparation\n",
    "\n",
    "•\tIn this step the data is loaded from the given CSV files. <br>\n",
    "•\tTarget variables are OneHotEncoded so as to add the final predictions into the submissions file. <br>\n",
    "•\tSetting up of a specific image size (90x90). <br>\n",
    "•\tCreating lists for train, valid and test and then reading in the image files from test and train folders provided by the dataset and appending them to the list created. <br>\n",
    "•\tConverting the lists containing the image data as arrays. <br>\n",
    "•\tSplitting up the training set into train and validation sets. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OY5ufI2BHpZH"
   },
   "source": [
    "- #### Reading the csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qn5ySe2LHpZM"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('labels.csv')\n",
    "df_test = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mAJ_uSjRHpZX"
   },
   "source": [
    "#### Displaying the contents of training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "8tEly4OiHpZa",
    "outputId": "ecb7c661-aa82-4827-9aec-a04cce20e15a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>breed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000bec180eb18c7604dcecc8fe0dba07</td>\n",
       "      <td>boston_bull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001513dfcb2ffafc82cccf4d8bbaba97</td>\n",
       "      <td>dingo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001cdf01b096e06d78e9e5112d419397</td>\n",
       "      <td>pekinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00214f311d5d2247d5dfe4fe24b2303d</td>\n",
       "      <td>bluetick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0021f9ceb3235effd7fcde7f7538ed62</td>\n",
       "      <td>golden_retriever</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id             breed\n",
       "0  000bec180eb18c7604dcecc8fe0dba07       boston_bull\n",
       "1  001513dfcb2ffafc82cccf4d8bbaba97             dingo\n",
       "2  001cdf01b096e06d78e9e5112d419397          pekinese\n",
       "3  00214f311d5d2247d5dfe4fe24b2303d          bluetick\n",
       "4  0021f9ceb3235effd7fcde7f7538ed62  golden_retriever"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gDLf2ZmoHpZl"
   },
   "source": [
    " - #### Displaying the contents of test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nc7i3FR3HpZo"
   },
   "outputs": [],
   "source": [
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ccd9ZUfcHpZv"
   },
   "source": [
    "- #### Using OneHotEncoding so as to put the outputs in submissions file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J9Wyf-t1HpZy"
   },
   "outputs": [],
   "source": [
    "#the breed needs to be one-hot encoded for the final submission\n",
    "targets = pd.Series(df_train['breed'])\n",
    "one_hot = pd.get_dummies(targets, sparse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OwovSan0HpZ8"
   },
   "outputs": [],
   "source": [
    "# Converting the encoded file as array\n",
    "one_hot_labels = np.asarray(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "id": "y2MCldX5HpaC",
    "outputId": "10c289d2-bb82-4ea1-b6d3-390f0c408d96"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8)"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_labels[99]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ysZ1DdamHpaN"
   },
   "source": [
    "- #### Reading the images from test and train folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "59KelopFHpaP"
   },
   "outputs": [],
   "source": [
    "image_size = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4W9Wy7ETHpaW"
   },
   "outputs": [],
   "source": [
    "train = []\n",
    "valid = []\n",
    "test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ldr1cHd5Hpad",
    "outputId": "e90369c9-73ee-441e-e80e-0d1625d0f1da"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10222/10222 [00:22<00:00, 460.57it/s]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "#   id  breed       train set\n",
    "for f, breed in tqdm(df_train.values):\n",
    "    img = cv2.imread('train/{}.jpg'.format(f))\n",
    "    label = one_hot_labels[i]\n",
    "    train.append(cv2.resize(img, (image_size, image_size)))\n",
    "    valid.append(label)\n",
    "    i += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "3wHnuHOJHpam",
    "outputId": "634d21d0-4971-45a0-c05f-44e2b31c63c8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10357/10357 [00:21<00:00, 473.97it/s]\n"
     ]
    }
   ],
   "source": [
    "for f in tqdm(df_test['id'].values):\n",
    "    img = cv2.imread('test/{}.jpg'.format(f))\n",
    "    test.append(cv2.resize(img, (image_size, image_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dY1wErxPHpat"
   },
   "source": [
    "- #### Converting Train, Test, Valid to Array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RXiUSO3CHpau"
   },
   "outputs": [],
   "source": [
    "train_ar = np.array(train, np.float32) / 255 #Bring it to the pixel value\n",
    "valid_ar = np.array(valid, np.uint8)\n",
    "test_ar = np.array(test, np.float32) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "3kgqgdShHpa1",
    "outputId": "022ff86d-154d-427b-aef0-0fd6110d2369"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10222, 90, 90, 3)\n",
      "(10222, 120)\n",
      "(10357, 90, 90, 3)\n"
     ]
    }
   ],
   "source": [
    "# Checking the shaleps of array elements\n",
    "print(train_ar.shape)\n",
    "print(valid_ar.shape)\n",
    "print(test_ar.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dbXRXnsoHpa9"
   },
   "source": [
    "- #### As Validation set contains 120 different class items, it can separated and be used while building a CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hwa3V1gkHpa-"
   },
   "outputs": [],
   "source": [
    "no_classes = valid_ar.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nww1oCZpHpbD"
   },
   "source": [
    "- #### Splitting the Train and Valid sets into splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NzuWf7ssHpbG"
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, Y_train, Y_valid = train_test_split(train_ar, valid_ar, test_size = 0.3, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "cj_TqJjoHpbM",
    "outputId": "61c1b2c2-d4d3-4666-8e9b-38f256a537b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7155, 90, 90, 3)\n",
      "(3067, 90, 90, 3)\n",
      "(7155, 120)\n",
      "(3067, 120)\n"
     ]
    }
   ],
   "source": [
    "# Verifying the shapes of the splits\n",
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-JaZnRtZHpbR"
   },
   "source": [
    "## Step 3 - Building the CNN Model\n",
    "\n",
    "•\tImporting the important libraries. <br>\n",
    "•\tBuilding the Sequential model. <br>\n",
    "•\tAdding a convolutional layer with 64 filters followed by a max pool layer. <br>\n",
    "•\tAdding another conv layer with 32 filters followed by a max pool layer. <br>\n",
    "•\tAdding the last conv layer with 128 filters followed by a max pool layer. <br>\n",
    "•\tAdding a flatten layer to flatten the images. <br>\n",
    "•\tThen finally adding a fully connected layer. <br>\n",
    "•\tEpochs used are 32 inorder to watch the performance of the model. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3WZxpNOxHpbT"
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qtyiujIOUnff"
   },
   "source": [
    "- ### This model is trained without any pre-trained weights. The use of a very small learning rate is to reduced the validation loss through each epoch. \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 753
    },
    "colab_type": "code",
    "id": "A6or68AaZ94D",
    "outputId": "0740fbe5-ee13-4685-ca23-fc3a54be0ca8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 88, 88, 64)        1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 44, 44, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 44, 44, 32)        18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 22, 22, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 22, 22, 128)       36992     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 11, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 15488)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1982592   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 120)               15480     \n",
      "=================================================================\n",
      "Total params: 2,055,320\n",
      "Trainable params: 2,055,320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), input_shape=(90, 90, 3..., activation=\"relu\")`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "optimiser = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "\n",
    "# Initialising the CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Convolution and adding a maxpool layer\n",
    "classifier.add(Conv2D(64, 3, 3, input_shape = (image_size, image_size, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "classifier.add(Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\"))\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "classifier.add(Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\"))\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "# Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Full connection \n",
    "classifier.add(Dense(units = 128, activation = 'relu'))  # \"Hidden Layer\"\n",
    "classifier.add(Dropout(0.25))\n",
    "classifier.add(Dropout(0.25))\n",
    "classifier.add(Dropout(0.25))\n",
    "classifier.add(Dense(units = no_classes, activation = 'softmax')) # Softmax because the outcome has a 120 classes. \"OutPut Layer\".\n",
    "\n",
    "# Compiling the CNN\n",
    "classifier.compile(loss='categorical_crossentropy',\n",
    "                   optimizer = optimiser,\n",
    "                   metrics=['accuracy'])\n",
    "callbacks_list = [keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, verbose=1)]\n",
    "\n",
    "classifier.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iJ9oqa1vUnfw"
   },
   "source": [
    "- ### In order to prevent overfitting, early stopping is used to customize the learning rate on each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1304
    },
    "colab_type": "code",
    "id": "YYa2ZhE8h-Ih",
    "outputId": "19df1d4c-3c23-49f8-bffb-1783f6b52c89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 7155 samples, validate on 3067 samples\n",
      "Epoch 1/32\n",
      "7155/7155 [==============================] - 7s 1ms/step - loss: 4.7899 - acc: 0.0078 - val_loss: 4.7863 - val_acc: 0.0085\n",
      "Epoch 2/32\n",
      "7155/7155 [==============================] - 4s 547us/step - loss: 4.7851 - acc: 0.0105 - val_loss: 4.7835 - val_acc: 0.0117\n",
      "Epoch 3/32\n",
      "7155/7155 [==============================] - 4s 553us/step - loss: 4.7787 - acc: 0.0124 - val_loss: 4.7691 - val_acc: 0.0170\n",
      "Epoch 4/32\n",
      "7155/7155 [==============================] - 4s 550us/step - loss: 4.7597 - acc: 0.0133 - val_loss: 4.7393 - val_acc: 0.0222\n",
      "Epoch 5/32\n",
      "7155/7155 [==============================] - 4s 552us/step - loss: 4.7274 - acc: 0.0186 - val_loss: 4.7067 - val_acc: 0.0248\n",
      "Epoch 6/32\n",
      "7155/7155 [==============================] - 4s 555us/step - loss: 4.6861 - acc: 0.0249 - val_loss: 4.6527 - val_acc: 0.0303\n",
      "Epoch 7/32\n",
      "7155/7155 [==============================] - 4s 556us/step - loss: 4.6321 - acc: 0.0275 - val_loss: 4.6146 - val_acc: 0.0355\n",
      "Epoch 8/32\n",
      "7155/7155 [==============================] - 4s 574us/step - loss: 4.5865 - acc: 0.0348 - val_loss: 4.5850 - val_acc: 0.0388\n",
      "Epoch 9/32\n",
      "7155/7155 [==============================] - 4s 575us/step - loss: 4.5303 - acc: 0.0393 - val_loss: 4.5473 - val_acc: 0.0456\n",
      "Epoch 10/32\n",
      "7155/7155 [==============================] - 4s 562us/step - loss: 4.4709 - acc: 0.0453 - val_loss: 4.5191 - val_acc: 0.0489\n",
      "Epoch 11/32\n",
      "7155/7155 [==============================] - 4s 559us/step - loss: 4.4220 - acc: 0.0484 - val_loss: 4.4951 - val_acc: 0.0496\n",
      "Epoch 12/32\n",
      "7155/7155 [==============================] - 4s 555us/step - loss: 4.3698 - acc: 0.0584 - val_loss: 4.4656 - val_acc: 0.0545\n",
      "Epoch 13/32\n",
      "7155/7155 [==============================] - 4s 549us/step - loss: 4.3244 - acc: 0.0629 - val_loss: 4.4472 - val_acc: 0.0541\n",
      "Epoch 14/32\n",
      "7155/7155 [==============================] - 4s 547us/step - loss: 4.2745 - acc: 0.0651 - val_loss: 4.4375 - val_acc: 0.0486\n",
      "Epoch 15/32\n",
      "7155/7155 [==============================] - 4s 546us/step - loss: 4.2132 - acc: 0.0696 - val_loss: 4.4263 - val_acc: 0.0509\n",
      "Epoch 16/32\n",
      "7155/7155 [==============================] - 4s 552us/step - loss: 4.1498 - acc: 0.0808 - val_loss: 4.4192 - val_acc: 0.0489\n",
      "Epoch 17/32\n",
      "7155/7155 [==============================] - 4s 546us/step - loss: 4.0931 - acc: 0.0841 - val_loss: 4.4054 - val_acc: 0.0558\n",
      "Epoch 18/32\n",
      "7155/7155 [==============================] - 4s 548us/step - loss: 4.0091 - acc: 0.0970 - val_loss: 4.3995 - val_acc: 0.0496\n",
      "Epoch 19/32\n",
      "7155/7155 [==============================] - 4s 552us/step - loss: 3.9596 - acc: 0.1055 - val_loss: 4.3841 - val_acc: 0.0545\n",
      "Epoch 20/32\n",
      "7155/7155 [==============================] - 4s 551us/step - loss: 3.8628 - acc: 0.1164 - val_loss: 4.3937 - val_acc: 0.0535\n",
      "Epoch 21/32\n",
      "7155/7155 [==============================] - 4s 551us/step - loss: 3.7970 - acc: 0.1262 - val_loss: 4.3855 - val_acc: 0.0554\n",
      "Epoch 22/32\n",
      "7155/7155 [==============================] - 4s 549us/step - loss: 3.7361 - acc: 0.1319 - val_loss: 4.3845 - val_acc: 0.0597\n",
      "Epoch 23/32\n",
      "7155/7155 [==============================] - 4s 550us/step - loss: 3.6438 - acc: 0.1451 - val_loss: 4.3895 - val_acc: 0.0567\n",
      "Epoch 24/32\n",
      "7155/7155 [==============================] - 4s 550us/step - loss: 3.5721 - acc: 0.1535 - val_loss: 4.3984 - val_acc: 0.0571\n",
      "Epoch 25/32\n",
      "7155/7155 [==============================] - 4s 551us/step - loss: 3.4776 - acc: 0.1789 - val_loss: 4.4000 - val_acc: 0.0606\n",
      "Epoch 26/32\n",
      "7155/7155 [==============================] - 4s 552us/step - loss: 3.4009 - acc: 0.1873 - val_loss: 4.4220 - val_acc: 0.0590\n",
      "Epoch 27/32\n",
      "7155/7155 [==============================] - 4s 568us/step - loss: 3.3151 - acc: 0.1961 - val_loss: 4.4331 - val_acc: 0.0626\n",
      "Epoch 28/32\n",
      "7155/7155 [==============================] - 4s 574us/step - loss: 3.2265 - acc: 0.2110 - val_loss: 4.4303 - val_acc: 0.0590\n",
      "Epoch 29/32\n",
      "7155/7155 [==============================] - 4s 574us/step - loss: 3.1473 - acc: 0.2204 - val_loss: 4.4852 - val_acc: 0.0593\n",
      "Epoch 30/32\n",
      "7155/7155 [==============================] - 4s 564us/step - loss: 3.0575 - acc: 0.2394 - val_loss: 4.4860 - val_acc: 0.0623\n",
      "Epoch 31/32\n",
      "7155/7155 [==============================] - 4s 555us/step - loss: 2.9751 - acc: 0.2583 - val_loss: 4.5196 - val_acc: 0.0613\n",
      "Epoch 32/32\n",
      "7155/7155 [==============================] - 4s 551us/step - loss: 2.8814 - acc: 0.2710 - val_loss: 4.5472 - val_acc: 0.0590\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fed0453ce10>"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import Callback\n",
    "\n",
    "class TerminateOnBaseline(Callback):\n",
    "    \"\"\"Callback that terminates training when either acc or val_acc reaches a specified baseline\n",
    "    \"\"\"\n",
    "    def __init__(self, monitor='acc', baseline=0.9):\n",
    "        super(TerminateOnBaseline, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.baseline = baseline\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        acc = logs.get(self.monitor)\n",
    "        if acc is not None:\n",
    "            if acc >= self.baseline:\n",
    "                print('Epoch %d: Reached baseline, terminating training' % (epoch))\n",
    "                self.model.stop_training = True\n",
    "                \n",
    "classifier.fit(X_train, Y_train, epochs = 32, validation_data = (X_valid, Y_valid), verbose=1, callbacks=[TerminateOnBaseline(monitor='acc', baseline=0.90)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dIKamfwOaD5H"
   },
   "source": [
    "## ResNet50 model with pre-trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Uc1hukJdE_Z"
   },
   "outputs": [],
   "source": [
    "#  Unzipping the h5 weights file.\n",
    "!unzip -q \"drive/My Drive/dataset/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "colab_type": "code",
    "id": "-RHc36OjaCO0",
    "outputId": "84f1d699-d099-43af-d563-2fbe8fdec9e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 2048)              23587712  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120)               61560     \n",
      "=================================================================\n",
      "Total params: 24,698,360\n",
      "Trainable params: 1,110,648\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.applications import ResNet50\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Flatten, GlobalAveragePooling2D, Activation, Flatten, Dropout, BatchNormalization\n",
    "import h5py\n",
    "\n",
    "\n",
    "\n",
    "#resnet_weights_path = 'resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "my_new_model = Sequential()\n",
    "my_new_model.add(ResNet50(include_top=False, pooling='avg', weights='resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'))\n",
    "my_new_model.add(Dense(512))\n",
    "my_new_model.add(Activation('relu'))\n",
    "my_new_model.add(Dropout(0.5))\n",
    "my_new_model.add(Dense(no_classes, activation='softmax'))\n",
    "\n",
    "# Say not to train first layer (ResNet) model. It is already trained\n",
    "my_new_model.layers[0].trainable = False\n",
    "\n",
    "my_new_model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "my_new_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1229
    },
    "colab_type": "code",
    "id": "b3-5p_vGhfB-",
    "outputId": "d1d7009d-86f8-4a73-ea9f-e6b1e6131234"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7155 samples, validate on 3067 samples\n",
      "Epoch 1/32\n",
      "7155/7155 [==============================] - 16s 2ms/sample - loss: 5.5056 - accuracy: 0.0080 - val_loss: 8.6706 - val_accuracy: 0.0095\n",
      "Epoch 2/32\n",
      "7155/7155 [==============================] - 9s 1ms/sample - loss: 5.2143 - accuracy: 0.0119 - val_loss: 9.6978 - val_accuracy: 0.0114\n",
      "Epoch 3/32\n",
      "7155/7155 [==============================] - 9s 1ms/sample - loss: 5.0735 - accuracy: 0.0150 - val_loss: 5.3506 - val_accuracy: 0.0078\n",
      "Epoch 4/32\n",
      "7155/7155 [==============================] - 9s 1ms/sample - loss: 4.9534 - accuracy: 0.0168 - val_loss: 4.9547 - val_accuracy: 0.0108\n",
      "Epoch 5/32\n",
      "7155/7155 [==============================] - 9s 1ms/sample - loss: 4.8653 - accuracy: 0.0203 - val_loss: 4.7809 - val_accuracy: 0.0218\n",
      "Epoch 6/32\n",
      "7155/7155 [==============================] - 9s 1ms/sample - loss: 4.8032 - accuracy: 0.0233 - val_loss: 4.7136 - val_accuracy: 0.0271\n",
      "Epoch 7/32\n",
      "7155/7155 [==============================] - 9s 1ms/sample - loss: 4.7537 - accuracy: 0.0280 - val_loss: 4.6786 - val_accuracy: 0.0310\n",
      "Epoch 8/32\n",
      "7155/7155 [==============================] - 9s 1ms/sample - loss: 4.7076 - accuracy: 0.0326 - val_loss: 4.6462 - val_accuracy: 0.0375\n",
      "Epoch 9/32\n",
      "7155/7155 [==============================] - 9s 1ms/sample - loss: 4.6651 - accuracy: 0.0384 - val_loss: 4.6193 - val_accuracy: 0.0401\n",
      "Epoch 10/32\n",
      "7155/7155 [==============================] - 9s 1ms/sample - loss: 4.6079 - accuracy: 0.0419 - val_loss: 4.5912 - val_accuracy: 0.0443\n",
      "Epoch 11/32\n",
      "7155/7155 [==============================] - 9s 1ms/sample - loss: 4.5820 - accuracy: 0.0481 - val_loss: 4.5632 - val_accuracy: 0.0525\n",
      "Epoch 12/32\n",
      "7155/7155 [==============================] - 9s 1ms/sample - loss: 4.5500 - accuracy: 0.0491 - val_loss: 4.5363 - val_accuracy: 0.0571\n",
      "Epoch 13/32\n",
      "7155/7155 [==============================] - 9s 1ms/sample - loss: 4.5080 - accuracy: 0.0545 - val_loss: 4.5129 - val_accuracy: 0.0613\n",
      "Epoch 14/32\n",
      "7155/7155 [==============================] - 9s 1ms/sample - loss: 4.4916 - accuracy: 0.0612 - val_loss: 4.4851 - val_accuracy: 0.0665\n",
      "Epoch 15/32\n",
      "7155/7155 [==============================] - 9s 1ms/sample - loss: 4.4428 - accuracy: 0.0636 - val_loss: 4.4613 - val_accuracy: 0.0727\n",
      "Epoch 16/32\n",
      "7155/7155 [==============================] - 9s 1ms/sample - loss: 4.4209 - accuracy: 0.0654 - val_loss: 4.4298 - val_accuracy: 0.0756\n",
      "Epoch 17/32\n",
      "7155/7155 [==============================] - 9s 1ms/sample - loss: 4.3786 - accuracy: 0.0759 - val_loss: 4.4040 - val_accuracy: 0.0786\n",
      "Epoch 18/32\n",
      "7155/7155 [==============================] - 9s 1ms/sample - loss: 4.3494 - accuracy: 0.0801 - val_loss: 4.3767 - val_accuracy: 0.0822\n",
      "Epoch 19/32\n",
      "7155/7155 [==============================] - 9s 1ms/sample - loss: 4.3051 - accuracy: 0.0886 - val_loss: 4.3499 - val_accuracy: 0.0844\n",
      "Epoch 20/32\n",
      "7155/7155 [==============================] - 9s 1ms/sample - loss: 4.2597 - accuracy: 0.0943 - val_loss: 4.3243 - val_accuracy: 0.0897\n",
      "Epoch 21/32\n",
      "7155/7155 [==============================] - 9s 1ms/sample - loss: 4.2416 - accuracy: 0.0960 - val_loss: 4.2948 - val_accuracy: 0.0946\n",
      "Epoch 22/32\n",
      "7155/7155 [==============================] - 9s 1ms/sample - loss: 4.2166 - accuracy: 0.0962 - val_loss: 4.2665 - val_accuracy: 0.1001\n",
      "Epoch 23/32\n",
      "7155/7155 [==============================] - 9s 1ms/sample - loss: 4.1801 - accuracy: 0.1030 - val_loss: 4.2388 - val_accuracy: 0.1053\n",
      "Epoch 24/32\n",
      "7155/7155 [==============================] - 9s 1ms/sample - loss: 4.1373 - accuracy: 0.1099 - val_loss: 4.2103 - val_accuracy: 0.1082\n",
      "Epoch 25/32\n",
      "7155/7155 [==============================] - 9s 1ms/sample - loss: 4.0995 - accuracy: 0.1094 - val_loss: 4.1889 - val_accuracy: 0.1128\n",
      "Epoch 26/32\n",
      "7155/7155 [==============================] - 9s 1ms/sample - loss: 4.0765 - accuracy: 0.1181 - val_loss: 4.1631 - val_accuracy: 0.1167\n",
      "Epoch 27/32\n",
      "7155/7155 [==============================] - 9s 1ms/sample - loss: 4.0464 - accuracy: 0.1194 - val_loss: 4.1275 - val_accuracy: 0.1216\n",
      "Epoch 28/32\n",
      "7155/7155 [==============================] - 9s 1ms/sample - loss: 3.9976 - accuracy: 0.1308 - val_loss: 4.1098 - val_accuracy: 0.1236\n",
      "Epoch 29/32\n",
      "7155/7155 [==============================] - 9s 1ms/sample - loss: 3.9825 - accuracy: 0.1298 - val_loss: 4.0816 - val_accuracy: 0.1239\n",
      "Epoch 30/32\n",
      "7155/7155 [==============================] - 9s 1ms/sample - loss: 3.9487 - accuracy: 0.1403 - val_loss: 4.0594 - val_accuracy: 0.1275\n",
      "Epoch 31/32\n",
      "7155/7155 [==============================] - 9s 1ms/sample - loss: 3.9165 - accuracy: 0.1405 - val_loss: 4.0412 - val_accuracy: 0.1301\n",
      "Epoch 32/32\n",
      "7155/7155 [==============================] - 9s 1ms/sample - loss: 3.8828 - accuracy: 0.1507 - val_loss: 4.0153 - val_accuracy: 0.1360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f07e4c6b748>"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import Callback\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "stop = EarlyStopping(monitor='val_loss', patience=15, verbose=1, mode='min')\n",
    "\n",
    "\n",
    "                \n",
    "my_new_model.fit(X_train, Y_train, epochs = 32, validation_data = (X_valid, Y_valid), verbose=1, callbacks=[stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WN1zbOsLHpbx"
   },
   "source": [
    "### Step 4 - Predictions\n",
    "\n",
    "•\tPredicting the values of the classifier. <br>\n",
    "•\tCreating a new data frame and adding predicted values. <br>\n",
    "•\tSetting the column names as generated from OneHotEncoding. <br>\n",
    "•\tInserting the test set column id to the final submission file. <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "F4kzlKf7Hpbz",
    "outputId": "dc64e016-95c8-4f0f-8bbe-adea0c3e3b46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10357/10357 [==============================] - 10s 927us/sample\n"
     ]
    }
   ],
   "source": [
    "preds = my_new_model.predict(test_ar, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "THa32UZ96dwk"
   },
   "source": [
    "- #### Converting predictions to a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZBeNgdty4SV4"
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QB4FgYFK6ww8"
   },
   "source": [
    "- #### Set column names to those generated by the one-hot encoding earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z2yyGucAoZNF"
   },
   "outputs": [],
   "source": [
    "col_names = one_hot.columns.values\n",
    "sub.columns = col_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uTKek_Lq7KFu"
   },
   "source": [
    "- #### Insert the column id from the sample submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q0Stfg4z7KRi"
   },
   "outputs": [],
   "source": [
    "sub.insert(0, 'id', df_test['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions with pre-trained weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "colab_type": "code",
    "id": "5m3v23NIkn_v",
    "outputId": "7e37f703-b39b-4623-8e50-c37c15f4025b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>affenpinscher</th>\n",
       "      <th>afghan_hound</th>\n",
       "      <th>african_hunting_dog</th>\n",
       "      <th>airedale</th>\n",
       "      <th>american_staffordshire_terrier</th>\n",
       "      <th>appenzeller</th>\n",
       "      <th>australian_terrier</th>\n",
       "      <th>basenji</th>\n",
       "      <th>basset</th>\n",
       "      <th>beagle</th>\n",
       "      <th>bedlington_terrier</th>\n",
       "      <th>bernese_mountain_dog</th>\n",
       "      <th>black-and-tan_coonhound</th>\n",
       "      <th>blenheim_spaniel</th>\n",
       "      <th>bloodhound</th>\n",
       "      <th>bluetick</th>\n",
       "      <th>border_collie</th>\n",
       "      <th>border_terrier</th>\n",
       "      <th>borzoi</th>\n",
       "      <th>boston_bull</th>\n",
       "      <th>bouvier_des_flandres</th>\n",
       "      <th>boxer</th>\n",
       "      <th>brabancon_griffon</th>\n",
       "      <th>briard</th>\n",
       "      <th>brittany_spaniel</th>\n",
       "      <th>bull_mastiff</th>\n",
       "      <th>cairn</th>\n",
       "      <th>cardigan</th>\n",
       "      <th>chesapeake_bay_retriever</th>\n",
       "      <th>chihuahua</th>\n",
       "      <th>chow</th>\n",
       "      <th>clumber</th>\n",
       "      <th>cocker_spaniel</th>\n",
       "      <th>collie</th>\n",
       "      <th>curly-coated_retriever</th>\n",
       "      <th>dandie_dinmont</th>\n",
       "      <th>dhole</th>\n",
       "      <th>dingo</th>\n",
       "      <th>doberman</th>\n",
       "      <th>...</th>\n",
       "      <th>norwegian_elkhound</th>\n",
       "      <th>norwich_terrier</th>\n",
       "      <th>old_english_sheepdog</th>\n",
       "      <th>otterhound</th>\n",
       "      <th>papillon</th>\n",
       "      <th>pekinese</th>\n",
       "      <th>pembroke</th>\n",
       "      <th>pomeranian</th>\n",
       "      <th>pug</th>\n",
       "      <th>redbone</th>\n",
       "      <th>rhodesian_ridgeback</th>\n",
       "      <th>rottweiler</th>\n",
       "      <th>saint_bernard</th>\n",
       "      <th>saluki</th>\n",
       "      <th>samoyed</th>\n",
       "      <th>schipperke</th>\n",
       "      <th>scotch_terrier</th>\n",
       "      <th>scottish_deerhound</th>\n",
       "      <th>sealyham_terrier</th>\n",
       "      <th>shetland_sheepdog</th>\n",
       "      <th>shih-tzu</th>\n",
       "      <th>siberian_husky</th>\n",
       "      <th>silky_terrier</th>\n",
       "      <th>soft-coated_wheaten_terrier</th>\n",
       "      <th>staffordshire_bullterrier</th>\n",
       "      <th>standard_poodle</th>\n",
       "      <th>standard_schnauzer</th>\n",
       "      <th>sussex_spaniel</th>\n",
       "      <th>tibetan_mastiff</th>\n",
       "      <th>tibetan_terrier</th>\n",
       "      <th>toy_poodle</th>\n",
       "      <th>toy_terrier</th>\n",
       "      <th>vizsla</th>\n",
       "      <th>walker_hound</th>\n",
       "      <th>weimaraner</th>\n",
       "      <th>welsh_springer_spaniel</th>\n",
       "      <th>west_highland_white_terrier</th>\n",
       "      <th>whippet</th>\n",
       "      <th>wire-haired_fox_terrier</th>\n",
       "      <th>yorkshire_terrier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000621fb3cbb32d8935728e48679680e</td>\n",
       "      <td>0.003295</td>\n",
       "      <td>0.010073</td>\n",
       "      <td>0.006655</td>\n",
       "      <td>0.001397</td>\n",
       "      <td>0.005403</td>\n",
       "      <td>0.004580</td>\n",
       "      <td>0.019117</td>\n",
       "      <td>0.009447</td>\n",
       "      <td>0.015657</td>\n",
       "      <td>0.021782</td>\n",
       "      <td>0.012432</td>\n",
       "      <td>0.006409</td>\n",
       "      <td>0.001357</td>\n",
       "      <td>0.026229</td>\n",
       "      <td>0.002839</td>\n",
       "      <td>0.003175</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>0.001005</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>0.001849</td>\n",
       "      <td>0.001402</td>\n",
       "      <td>0.006751</td>\n",
       "      <td>0.002805</td>\n",
       "      <td>0.001610</td>\n",
       "      <td>0.015128</td>\n",
       "      <td>0.001629</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.002014</td>\n",
       "      <td>0.003871</td>\n",
       "      <td>0.011439</td>\n",
       "      <td>0.006583</td>\n",
       "      <td>0.008266</td>\n",
       "      <td>0.007522</td>\n",
       "      <td>0.011978</td>\n",
       "      <td>0.000607</td>\n",
       "      <td>0.010303</td>\n",
       "      <td>0.007315</td>\n",
       "      <td>0.003955</td>\n",
       "      <td>0.002696</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017492</td>\n",
       "      <td>0.010488</td>\n",
       "      <td>0.009110</td>\n",
       "      <td>0.005001</td>\n",
       "      <td>0.028970</td>\n",
       "      <td>0.040749</td>\n",
       "      <td>0.023176</td>\n",
       "      <td>0.015510</td>\n",
       "      <td>0.003546</td>\n",
       "      <td>0.000962</td>\n",
       "      <td>0.005938</td>\n",
       "      <td>0.001418</td>\n",
       "      <td>0.037140</td>\n",
       "      <td>0.004559</td>\n",
       "      <td>0.061934</td>\n",
       "      <td>0.002164</td>\n",
       "      <td>0.001857</td>\n",
       "      <td>0.004575</td>\n",
       "      <td>0.007565</td>\n",
       "      <td>0.012354</td>\n",
       "      <td>0.013495</td>\n",
       "      <td>0.016970</td>\n",
       "      <td>0.007115</td>\n",
       "      <td>0.002408</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>0.004046</td>\n",
       "      <td>0.001751</td>\n",
       "      <td>0.002125</td>\n",
       "      <td>0.000551</td>\n",
       "      <td>0.015168</td>\n",
       "      <td>0.002914</td>\n",
       "      <td>0.001850</td>\n",
       "      <td>0.003972</td>\n",
       "      <td>0.005894</td>\n",
       "      <td>0.001424</td>\n",
       "      <td>0.008413</td>\n",
       "      <td>0.009067</td>\n",
       "      <td>0.004233</td>\n",
       "      <td>0.005832</td>\n",
       "      <td>0.013386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00102ee9d8eb90812350685311fe5890</td>\n",
       "      <td>0.003557</td>\n",
       "      <td>0.001760</td>\n",
       "      <td>0.004654</td>\n",
       "      <td>0.001682</td>\n",
       "      <td>0.004975</td>\n",
       "      <td>0.002723</td>\n",
       "      <td>0.007730</td>\n",
       "      <td>0.001785</td>\n",
       "      <td>0.005749</td>\n",
       "      <td>0.007642</td>\n",
       "      <td>0.010738</td>\n",
       "      <td>0.014083</td>\n",
       "      <td>0.001414</td>\n",
       "      <td>0.006594</td>\n",
       "      <td>0.005503</td>\n",
       "      <td>0.001954</td>\n",
       "      <td>0.003611</td>\n",
       "      <td>0.005245</td>\n",
       "      <td>0.004157</td>\n",
       "      <td>0.006316</td>\n",
       "      <td>0.002280</td>\n",
       "      <td>0.007380</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.001403</td>\n",
       "      <td>0.011242</td>\n",
       "      <td>0.006891</td>\n",
       "      <td>0.002934</td>\n",
       "      <td>0.004457</td>\n",
       "      <td>0.003910</td>\n",
       "      <td>0.007762</td>\n",
       "      <td>0.009491</td>\n",
       "      <td>0.008867</td>\n",
       "      <td>0.005411</td>\n",
       "      <td>0.020947</td>\n",
       "      <td>0.003833</td>\n",
       "      <td>0.013014</td>\n",
       "      <td>0.005753</td>\n",
       "      <td>0.004338</td>\n",
       "      <td>0.001704</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015992</td>\n",
       "      <td>0.004086</td>\n",
       "      <td>0.017274</td>\n",
       "      <td>0.004386</td>\n",
       "      <td>0.011053</td>\n",
       "      <td>0.027466</td>\n",
       "      <td>0.011988</td>\n",
       "      <td>0.012787</td>\n",
       "      <td>0.008959</td>\n",
       "      <td>0.005610</td>\n",
       "      <td>0.004832</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>0.010672</td>\n",
       "      <td>0.005466</td>\n",
       "      <td>0.020205</td>\n",
       "      <td>0.002276</td>\n",
       "      <td>0.007874</td>\n",
       "      <td>0.005073</td>\n",
       "      <td>0.019006</td>\n",
       "      <td>0.015882</td>\n",
       "      <td>0.022870</td>\n",
       "      <td>0.014634</td>\n",
       "      <td>0.004497</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>0.007858</td>\n",
       "      <td>0.004461</td>\n",
       "      <td>0.006161</td>\n",
       "      <td>0.002990</td>\n",
       "      <td>0.014276</td>\n",
       "      <td>0.007284</td>\n",
       "      <td>0.008205</td>\n",
       "      <td>0.005989</td>\n",
       "      <td>0.005559</td>\n",
       "      <td>0.006060</td>\n",
       "      <td>0.006399</td>\n",
       "      <td>0.006834</td>\n",
       "      <td>0.020459</td>\n",
       "      <td>0.008513</td>\n",
       "      <td>0.002145</td>\n",
       "      <td>0.002953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0012a730dfa437f5f3613fb75efcd4ce</td>\n",
       "      <td>0.005690</td>\n",
       "      <td>0.025505</td>\n",
       "      <td>0.005594</td>\n",
       "      <td>0.011083</td>\n",
       "      <td>0.002817</td>\n",
       "      <td>0.008095</td>\n",
       "      <td>0.011933</td>\n",
       "      <td>0.007694</td>\n",
       "      <td>0.006999</td>\n",
       "      <td>0.006187</td>\n",
       "      <td>0.013324</td>\n",
       "      <td>0.004056</td>\n",
       "      <td>0.007396</td>\n",
       "      <td>0.008796</td>\n",
       "      <td>0.008430</td>\n",
       "      <td>0.006140</td>\n",
       "      <td>0.007109</td>\n",
       "      <td>0.006565</td>\n",
       "      <td>0.006203</td>\n",
       "      <td>0.008343</td>\n",
       "      <td>0.007632</td>\n",
       "      <td>0.007475</td>\n",
       "      <td>0.003423</td>\n",
       "      <td>0.006148</td>\n",
       "      <td>0.005868</td>\n",
       "      <td>0.004081</td>\n",
       "      <td>0.018638</td>\n",
       "      <td>0.008806</td>\n",
       "      <td>0.005144</td>\n",
       "      <td>0.005327</td>\n",
       "      <td>0.005144</td>\n",
       "      <td>0.004055</td>\n",
       "      <td>0.013826</td>\n",
       "      <td>0.008737</td>\n",
       "      <td>0.003818</td>\n",
       "      <td>0.010600</td>\n",
       "      <td>0.007538</td>\n",
       "      <td>0.008770</td>\n",
       "      <td>0.006291</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004677</td>\n",
       "      <td>0.006526</td>\n",
       "      <td>0.007149</td>\n",
       "      <td>0.007130</td>\n",
       "      <td>0.007390</td>\n",
       "      <td>0.007296</td>\n",
       "      <td>0.011114</td>\n",
       "      <td>0.003551</td>\n",
       "      <td>0.008402</td>\n",
       "      <td>0.010372</td>\n",
       "      <td>0.008928</td>\n",
       "      <td>0.004255</td>\n",
       "      <td>0.007639</td>\n",
       "      <td>0.010762</td>\n",
       "      <td>0.010714</td>\n",
       "      <td>0.003691</td>\n",
       "      <td>0.011524</td>\n",
       "      <td>0.012097</td>\n",
       "      <td>0.008209</td>\n",
       "      <td>0.006693</td>\n",
       "      <td>0.006070</td>\n",
       "      <td>0.015903</td>\n",
       "      <td>0.011199</td>\n",
       "      <td>0.007155</td>\n",
       "      <td>0.003196</td>\n",
       "      <td>0.008665</td>\n",
       "      <td>0.010056</td>\n",
       "      <td>0.012476</td>\n",
       "      <td>0.009351</td>\n",
       "      <td>0.013005</td>\n",
       "      <td>0.004552</td>\n",
       "      <td>0.008094</td>\n",
       "      <td>0.006391</td>\n",
       "      <td>0.008337</td>\n",
       "      <td>0.008658</td>\n",
       "      <td>0.014715</td>\n",
       "      <td>0.005416</td>\n",
       "      <td>0.009645</td>\n",
       "      <td>0.011179</td>\n",
       "      <td>0.010194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001510bc8570bbeee98c8d80c8a95ec1</td>\n",
       "      <td>0.007939</td>\n",
       "      <td>0.010598</td>\n",
       "      <td>0.003670</td>\n",
       "      <td>0.006984</td>\n",
       "      <td>0.005918</td>\n",
       "      <td>0.005126</td>\n",
       "      <td>0.006056</td>\n",
       "      <td>0.015743</td>\n",
       "      <td>0.001414</td>\n",
       "      <td>0.004458</td>\n",
       "      <td>0.009349</td>\n",
       "      <td>0.008005</td>\n",
       "      <td>0.009284</td>\n",
       "      <td>0.005038</td>\n",
       "      <td>0.018982</td>\n",
       "      <td>0.002256</td>\n",
       "      <td>0.007330</td>\n",
       "      <td>0.007330</td>\n",
       "      <td>0.010116</td>\n",
       "      <td>0.005640</td>\n",
       "      <td>0.016282</td>\n",
       "      <td>0.006722</td>\n",
       "      <td>0.003113</td>\n",
       "      <td>0.008094</td>\n",
       "      <td>0.002922</td>\n",
       "      <td>0.008009</td>\n",
       "      <td>0.009726</td>\n",
       "      <td>0.004785</td>\n",
       "      <td>0.009717</td>\n",
       "      <td>0.005435</td>\n",
       "      <td>0.002857</td>\n",
       "      <td>0.005875</td>\n",
       "      <td>0.009192</td>\n",
       "      <td>0.008229</td>\n",
       "      <td>0.011335</td>\n",
       "      <td>0.002590</td>\n",
       "      <td>0.001494</td>\n",
       "      <td>0.007554</td>\n",
       "      <td>0.011126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014079</td>\n",
       "      <td>0.020567</td>\n",
       "      <td>0.002718</td>\n",
       "      <td>0.012525</td>\n",
       "      <td>0.002474</td>\n",
       "      <td>0.001747</td>\n",
       "      <td>0.004789</td>\n",
       "      <td>0.005999</td>\n",
       "      <td>0.008278</td>\n",
       "      <td>0.013972</td>\n",
       "      <td>0.007615</td>\n",
       "      <td>0.005279</td>\n",
       "      <td>0.003601</td>\n",
       "      <td>0.008443</td>\n",
       "      <td>0.006433</td>\n",
       "      <td>0.011960</td>\n",
       "      <td>0.009164</td>\n",
       "      <td>0.007739</td>\n",
       "      <td>0.008486</td>\n",
       "      <td>0.005397</td>\n",
       "      <td>0.002835</td>\n",
       "      <td>0.007550</td>\n",
       "      <td>0.006284</td>\n",
       "      <td>0.007561</td>\n",
       "      <td>0.004664</td>\n",
       "      <td>0.013253</td>\n",
       "      <td>0.006855</td>\n",
       "      <td>0.006888</td>\n",
       "      <td>0.005391</td>\n",
       "      <td>0.010212</td>\n",
       "      <td>0.003891</td>\n",
       "      <td>0.008965</td>\n",
       "      <td>0.004832</td>\n",
       "      <td>0.008584</td>\n",
       "      <td>0.005987</td>\n",
       "      <td>0.016630</td>\n",
       "      <td>0.006659</td>\n",
       "      <td>0.016582</td>\n",
       "      <td>0.007554</td>\n",
       "      <td>0.004290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001a5f3114548acdefa3d4da05474c2e</td>\n",
       "      <td>0.018856</td>\n",
       "      <td>0.017828</td>\n",
       "      <td>0.006129</td>\n",
       "      <td>0.010720</td>\n",
       "      <td>0.008787</td>\n",
       "      <td>0.005422</td>\n",
       "      <td>0.013939</td>\n",
       "      <td>0.002740</td>\n",
       "      <td>0.003364</td>\n",
       "      <td>0.006053</td>\n",
       "      <td>0.008583</td>\n",
       "      <td>0.009733</td>\n",
       "      <td>0.006810</td>\n",
       "      <td>0.004523</td>\n",
       "      <td>0.015079</td>\n",
       "      <td>0.008420</td>\n",
       "      <td>0.003872</td>\n",
       "      <td>0.009533</td>\n",
       "      <td>0.007953</td>\n",
       "      <td>0.003995</td>\n",
       "      <td>0.011027</td>\n",
       "      <td>0.010115</td>\n",
       "      <td>0.007313</td>\n",
       "      <td>0.009171</td>\n",
       "      <td>0.004423</td>\n",
       "      <td>0.002992</td>\n",
       "      <td>0.018377</td>\n",
       "      <td>0.009258</td>\n",
       "      <td>0.008155</td>\n",
       "      <td>0.002319</td>\n",
       "      <td>0.003942</td>\n",
       "      <td>0.007374</td>\n",
       "      <td>0.006155</td>\n",
       "      <td>0.006420</td>\n",
       "      <td>0.009053</td>\n",
       "      <td>0.014063</td>\n",
       "      <td>0.007113</td>\n",
       "      <td>0.004654</td>\n",
       "      <td>0.003910</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011568</td>\n",
       "      <td>0.005799</td>\n",
       "      <td>0.003613</td>\n",
       "      <td>0.009247</td>\n",
       "      <td>0.006599</td>\n",
       "      <td>0.005616</td>\n",
       "      <td>0.005541</td>\n",
       "      <td>0.001911</td>\n",
       "      <td>0.013433</td>\n",
       "      <td>0.008503</td>\n",
       "      <td>0.004911</td>\n",
       "      <td>0.005363</td>\n",
       "      <td>0.002788</td>\n",
       "      <td>0.003193</td>\n",
       "      <td>0.005514</td>\n",
       "      <td>0.003387</td>\n",
       "      <td>0.007490</td>\n",
       "      <td>0.021446</td>\n",
       "      <td>0.009517</td>\n",
       "      <td>0.003874</td>\n",
       "      <td>0.006015</td>\n",
       "      <td>0.007073</td>\n",
       "      <td>0.012177</td>\n",
       "      <td>0.003665</td>\n",
       "      <td>0.008118</td>\n",
       "      <td>0.012036</td>\n",
       "      <td>0.010670</td>\n",
       "      <td>0.015067</td>\n",
       "      <td>0.016689</td>\n",
       "      <td>0.011800</td>\n",
       "      <td>0.005219</td>\n",
       "      <td>0.005340</td>\n",
       "      <td>0.012210</td>\n",
       "      <td>0.003609</td>\n",
       "      <td>0.008568</td>\n",
       "      <td>0.004494</td>\n",
       "      <td>0.005347</td>\n",
       "      <td>0.006560</td>\n",
       "      <td>0.006794</td>\n",
       "      <td>0.010937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  ...  yorkshire_terrier\n",
       "0  000621fb3cbb32d8935728e48679680e  ...           0.013386\n",
       "1  00102ee9d8eb90812350685311fe5890  ...           0.002953\n",
       "2  0012a730dfa437f5f3613fb75efcd4ce  ...           0.010194\n",
       "3  001510bc8570bbeee98c8d80c8a95ec1  ...           0.004290\n",
       "4  001a5f3114548acdefa3d4da05474c2e  ...           0.010937\n",
       "\n",
       "[5 rows x 121 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions without the Pre-Trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "colab_type": "code",
    "id": "5m3v23NIkn_v",
    "outputId": "784dd701-5559-473f-d0fb-22e9957a110c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>affenpinscher</th>\n",
       "      <th>afghan_hound</th>\n",
       "      <th>african_hunting_dog</th>\n",
       "      <th>airedale</th>\n",
       "      <th>american_staffordshire_terrier</th>\n",
       "      <th>appenzeller</th>\n",
       "      <th>australian_terrier</th>\n",
       "      <th>basenji</th>\n",
       "      <th>basset</th>\n",
       "      <th>beagle</th>\n",
       "      <th>bedlington_terrier</th>\n",
       "      <th>bernese_mountain_dog</th>\n",
       "      <th>black-and-tan_coonhound</th>\n",
       "      <th>blenheim_spaniel</th>\n",
       "      <th>bloodhound</th>\n",
       "      <th>bluetick</th>\n",
       "      <th>border_collie</th>\n",
       "      <th>border_terrier</th>\n",
       "      <th>borzoi</th>\n",
       "      <th>boston_bull</th>\n",
       "      <th>bouvier_des_flandres</th>\n",
       "      <th>boxer</th>\n",
       "      <th>brabancon_griffon</th>\n",
       "      <th>briard</th>\n",
       "      <th>brittany_spaniel</th>\n",
       "      <th>bull_mastiff</th>\n",
       "      <th>cairn</th>\n",
       "      <th>cardigan</th>\n",
       "      <th>chesapeake_bay_retriever</th>\n",
       "      <th>chihuahua</th>\n",
       "      <th>chow</th>\n",
       "      <th>clumber</th>\n",
       "      <th>cocker_spaniel</th>\n",
       "      <th>collie</th>\n",
       "      <th>curly-coated_retriever</th>\n",
       "      <th>dandie_dinmont</th>\n",
       "      <th>dhole</th>\n",
       "      <th>dingo</th>\n",
       "      <th>doberman</th>\n",
       "      <th>...</th>\n",
       "      <th>norwegian_elkhound</th>\n",
       "      <th>norwich_terrier</th>\n",
       "      <th>old_english_sheepdog</th>\n",
       "      <th>otterhound</th>\n",
       "      <th>papillon</th>\n",
       "      <th>pekinese</th>\n",
       "      <th>pembroke</th>\n",
       "      <th>pomeranian</th>\n",
       "      <th>pug</th>\n",
       "      <th>redbone</th>\n",
       "      <th>rhodesian_ridgeback</th>\n",
       "      <th>rottweiler</th>\n",
       "      <th>saint_bernard</th>\n",
       "      <th>saluki</th>\n",
       "      <th>samoyed</th>\n",
       "      <th>schipperke</th>\n",
       "      <th>scotch_terrier</th>\n",
       "      <th>scottish_deerhound</th>\n",
       "      <th>sealyham_terrier</th>\n",
       "      <th>shetland_sheepdog</th>\n",
       "      <th>shih-tzu</th>\n",
       "      <th>siberian_husky</th>\n",
       "      <th>silky_terrier</th>\n",
       "      <th>soft-coated_wheaten_terrier</th>\n",
       "      <th>staffordshire_bullterrier</th>\n",
       "      <th>standard_poodle</th>\n",
       "      <th>standard_schnauzer</th>\n",
       "      <th>sussex_spaniel</th>\n",
       "      <th>tibetan_mastiff</th>\n",
       "      <th>tibetan_terrier</th>\n",
       "      <th>toy_poodle</th>\n",
       "      <th>toy_terrier</th>\n",
       "      <th>vizsla</th>\n",
       "      <th>walker_hound</th>\n",
       "      <th>weimaraner</th>\n",
       "      <th>welsh_springer_spaniel</th>\n",
       "      <th>west_highland_white_terrier</th>\n",
       "      <th>whippet</th>\n",
       "      <th>wire-haired_fox_terrier</th>\n",
       "      <th>yorkshire_terrier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000621fb3cbb32d8935728e48679680e</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.013971</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000858</td>\n",
       "      <td>0.003813</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.006695</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>0.014394</td>\n",
       "      <td>0.020592</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.064436</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.001574</td>\n",
       "      <td>0.001759</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.035655</td>\n",
       "      <td>0.001163</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.006584</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>0.001428</td>\n",
       "      <td>0.001147</td>\n",
       "      <td>0.003018</td>\n",
       "      <td>0.002269</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000792</td>\n",
       "      <td>0.005230</td>\n",
       "      <td>0.028647</td>\n",
       "      <td>0.008660</td>\n",
       "      <td>0.004092</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.017674</td>\n",
       "      <td>0.001481</td>\n",
       "      <td>0.005466</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001257</td>\n",
       "      <td>0.000970</td>\n",
       "      <td>0.076155</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>0.018806</td>\n",
       "      <td>0.008374</td>\n",
       "      <td>0.007881</td>\n",
       "      <td>0.008280</td>\n",
       "      <td>0.011740</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>0.015982</td>\n",
       "      <td>0.035886</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.052842</td>\n",
       "      <td>0.016581</td>\n",
       "      <td>0.017982</td>\n",
       "      <td>0.006419</td>\n",
       "      <td>0.001365</td>\n",
       "      <td>0.005385</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.001462</td>\n",
       "      <td>0.000705</td>\n",
       "      <td>0.000497</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.002646</td>\n",
       "      <td>0.008935</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.005126</td>\n",
       "      <td>0.007172</td>\n",
       "      <td>0.018828</td>\n",
       "      <td>0.003084</td>\n",
       "      <td>0.002796</td>\n",
       "      <td>0.032529</td>\n",
       "      <td>0.001534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00102ee9d8eb90812350685311fe5890</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.001958</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.001605</td>\n",
       "      <td>0.001420</td>\n",
       "      <td>0.009206</td>\n",
       "      <td>0.037322</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000395</td>\n",
       "      <td>0.001425</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.008783</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000810</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.028797</td>\n",
       "      <td>0.002731</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.018004</td>\n",
       "      <td>0.002664</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.005708</td>\n",
       "      <td>0.008304</td>\n",
       "      <td>0.019577</td>\n",
       "      <td>0.002829</td>\n",
       "      <td>0.059393</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.002430</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>0.000639</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>0.004510</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>0.009064</td>\n",
       "      <td>0.017134</td>\n",
       "      <td>0.001550</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.001247</td>\n",
       "      <td>0.002791</td>\n",
       "      <td>0.017626</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.001602</td>\n",
       "      <td>0.018986</td>\n",
       "      <td>0.004305</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.000597</td>\n",
       "      <td>0.000834</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.001804</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.008806</td>\n",
       "      <td>0.010847</td>\n",
       "      <td>0.001776</td>\n",
       "      <td>0.004702</td>\n",
       "      <td>0.000520</td>\n",
       "      <td>0.003388</td>\n",
       "      <td>0.003639</td>\n",
       "      <td>0.043554</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.001210</td>\n",
       "      <td>0.061125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0012a730dfa437f5f3613fb75efcd4ce</td>\n",
       "      <td>0.000601</td>\n",
       "      <td>0.024437</td>\n",
       "      <td>0.002293</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>0.009814</td>\n",
       "      <td>0.001633</td>\n",
       "      <td>0.000901</td>\n",
       "      <td>0.016347</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.046961</td>\n",
       "      <td>0.072647</td>\n",
       "      <td>0.008292</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>0.088488</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.002126</td>\n",
       "      <td>0.011486</td>\n",
       "      <td>0.004057</td>\n",
       "      <td>0.013830</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.008847</td>\n",
       "      <td>0.000359</td>\n",
       "      <td>0.001478</td>\n",
       "      <td>0.009322</td>\n",
       "      <td>0.004909</td>\n",
       "      <td>0.008207</td>\n",
       "      <td>0.005839</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.001628</td>\n",
       "      <td>0.006941</td>\n",
       "      <td>0.002397</td>\n",
       "      <td>0.008535</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>0.012536</td>\n",
       "      <td>0.006824</td>\n",
       "      <td>0.010402</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003709</td>\n",
       "      <td>0.002533</td>\n",
       "      <td>0.031415</td>\n",
       "      <td>0.005594</td>\n",
       "      <td>0.008123</td>\n",
       "      <td>0.003223</td>\n",
       "      <td>0.003850</td>\n",
       "      <td>0.011990</td>\n",
       "      <td>0.015065</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.001083</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>0.023355</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.016597</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>0.000586</td>\n",
       "      <td>0.003290</td>\n",
       "      <td>0.008131</td>\n",
       "      <td>0.036888</td>\n",
       "      <td>0.019641</td>\n",
       "      <td>0.016742</td>\n",
       "      <td>0.014132</td>\n",
       "      <td>0.003437</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.001876</td>\n",
       "      <td>0.003019</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.005384</td>\n",
       "      <td>0.005639</td>\n",
       "      <td>0.004542</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.014175</td>\n",
       "      <td>0.007147</td>\n",
       "      <td>0.047418</td>\n",
       "      <td>0.003654</td>\n",
       "      <td>0.005983</td>\n",
       "      <td>0.043845</td>\n",
       "      <td>0.000954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001510bc8570bbeee98c8d80c8a95ec1</td>\n",
       "      <td>0.004299</td>\n",
       "      <td>0.026309</td>\n",
       "      <td>0.031833</td>\n",
       "      <td>0.037659</td>\n",
       "      <td>0.001723</td>\n",
       "      <td>0.008537</td>\n",
       "      <td>0.045453</td>\n",
       "      <td>0.001893</td>\n",
       "      <td>0.004110</td>\n",
       "      <td>0.002406</td>\n",
       "      <td>0.001335</td>\n",
       "      <td>0.001524</td>\n",
       "      <td>0.002575</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.002684</td>\n",
       "      <td>0.025355</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>0.015945</td>\n",
       "      <td>0.000583</td>\n",
       "      <td>0.010560</td>\n",
       "      <td>0.005364</td>\n",
       "      <td>0.002578</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.019001</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.001078</td>\n",
       "      <td>0.059912</td>\n",
       "      <td>0.011950</td>\n",
       "      <td>0.018248</td>\n",
       "      <td>0.001935</td>\n",
       "      <td>0.008891</td>\n",
       "      <td>0.001112</td>\n",
       "      <td>0.008133</td>\n",
       "      <td>0.002480</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.007544</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.002285</td>\n",
       "      <td>0.008481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012109</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.001982</td>\n",
       "      <td>0.003689</td>\n",
       "      <td>0.004243</td>\n",
       "      <td>0.002276</td>\n",
       "      <td>0.002316</td>\n",
       "      <td>0.002342</td>\n",
       "      <td>0.014669</td>\n",
       "      <td>0.002904</td>\n",
       "      <td>0.000919</td>\n",
       "      <td>0.001038</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.003587</td>\n",
       "      <td>0.000736</td>\n",
       "      <td>0.024009</td>\n",
       "      <td>0.014748</td>\n",
       "      <td>0.033917</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>0.002138</td>\n",
       "      <td>0.006467</td>\n",
       "      <td>0.001976</td>\n",
       "      <td>0.005961</td>\n",
       "      <td>0.008453</td>\n",
       "      <td>0.008915</td>\n",
       "      <td>0.003772</td>\n",
       "      <td>0.016621</td>\n",
       "      <td>0.002521</td>\n",
       "      <td>0.014216</td>\n",
       "      <td>0.005305</td>\n",
       "      <td>0.001063</td>\n",
       "      <td>0.005585</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.001298</td>\n",
       "      <td>0.016612</td>\n",
       "      <td>0.004480</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.005170</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.011247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001a5f3114548acdefa3d4da05474c2e</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.023421</td>\n",
       "      <td>0.005327</td>\n",
       "      <td>0.014026</td>\n",
       "      <td>0.007906</td>\n",
       "      <td>0.024775</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.006889</td>\n",
       "      <td>0.013063</td>\n",
       "      <td>0.036002</td>\n",
       "      <td>0.005281</td>\n",
       "      <td>0.006517</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.029085</td>\n",
       "      <td>0.000419</td>\n",
       "      <td>0.019778</td>\n",
       "      <td>0.001719</td>\n",
       "      <td>0.003568</td>\n",
       "      <td>0.003911</td>\n",
       "      <td>0.033228</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.001559</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.004389</td>\n",
       "      <td>0.010097</td>\n",
       "      <td>0.000787</td>\n",
       "      <td>0.007492</td>\n",
       "      <td>0.013489</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.002203</td>\n",
       "      <td>0.003769</td>\n",
       "      <td>0.026746</td>\n",
       "      <td>0.001749</td>\n",
       "      <td>0.011170</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.004189</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>0.014894</td>\n",
       "      <td>0.000658</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012247</td>\n",
       "      <td>0.000926</td>\n",
       "      <td>0.002993</td>\n",
       "      <td>0.004025</td>\n",
       "      <td>0.021586</td>\n",
       "      <td>0.006231</td>\n",
       "      <td>0.017769</td>\n",
       "      <td>0.001121</td>\n",
       "      <td>0.010030</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.018013</td>\n",
       "      <td>0.006774</td>\n",
       "      <td>0.004463</td>\n",
       "      <td>0.001422</td>\n",
       "      <td>0.000575</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.018233</td>\n",
       "      <td>0.007123</td>\n",
       "      <td>0.003295</td>\n",
       "      <td>0.023983</td>\n",
       "      <td>0.001584</td>\n",
       "      <td>0.002864</td>\n",
       "      <td>0.002479</td>\n",
       "      <td>0.000708</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>0.000273</td>\n",
       "      <td>0.001941</td>\n",
       "      <td>0.002122</td>\n",
       "      <td>0.015120</td>\n",
       "      <td>0.000790</td>\n",
       "      <td>0.004950</td>\n",
       "      <td>0.020391</td>\n",
       "      <td>0.005584</td>\n",
       "      <td>0.005239</td>\n",
       "      <td>0.005683</td>\n",
       "      <td>0.000732</td>\n",
       "      <td>0.008165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  ...  yorkshire_terrier\n",
       "0  000621fb3cbb32d8935728e48679680e  ...           0.001534\n",
       "1  00102ee9d8eb90812350685311fe5890  ...           0.061125\n",
       "2  0012a730dfa437f5f3613fb75efcd4ce  ...           0.000954\n",
       "3  001510bc8570bbeee98c8d80c8a95ec1  ...           0.011247\n",
       "4  001a5f3114548acdefa3d4da05474c2e  ...           0.008165\n",
       "\n",
       "[5 rows x 121 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Final_CNN_colab.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
